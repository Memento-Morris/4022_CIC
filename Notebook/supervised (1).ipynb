{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "67eb6460",
      "metadata": {
        "id": "67eb6460"
      },
      "source": [
        "# Supervised Learning Models for Anomaly-Based Intrusion Detection\n",
        "\n",
        "This Jupyter Notebook focuses on training and evaluating supervised machine learning models for anomaly-based intrusion detection. It builds upon the preprocessed CICIDS2017 dataset, prepared in the **data preprocessing notebook\n",
        "\n",
        "The broader goal is to develop a Network Intrusion Detection System (NIDS) prototype capable of identifying a range of network attacks, such as DoS, PortScan, and Brute Force, while balancing detection accuracy with computational efficiency. This is particularly critical in resource-constrained environments, focus of the project.\n",
        "\n",
        "**Models Being Evaluated:**\n",
        "\n",
        "* **Supervised Learning:**\n",
        "    * Random Forest\n",
        "    * XGBoost\n",
        "    * _k_-NN\n",
        "\n",
        "**Evaluation Strategy:**\n",
        "\n",
        "The models are assessed using k-fold cross-validation on the training data, with a separate hold-out test set reserved for final evaluation. Key performance metrics include accuracy, precision, recall, F1-score, and resource usage (CPU time and memory consumption). These metrics provide critical insights into algorithm efficiency and effectiveness, essential for real-time deployment in resource-constrained networks.\n",
        "\n",
        "This notebook presents the training process, hyperparameter tuning, and comparative analysis of the selected supervised learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GjMyHyG2Abac",
      "metadata": {
        "id": "GjMyHyG2Abac"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GA1U1AcGAyCb",
      "metadata": {
        "id": "GA1U1AcGAyCb"
      },
      "outputs": [],
      "source": [
        " drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3401c05",
      "metadata": {
        "id": "c3401c05",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Installing extra components\n",
        "!pip install memory_profiler\n",
        "!pip install psutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f89a850c",
      "metadata": {
        "id": "f89a850c"
      },
      "outputs": [],
      "source": [
        "# Importing the relevant libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import time\n",
        "import psutil\n",
        "import threading\n",
        "from memory_profiler import memory_usage\n",
        "\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b898acb",
      "metadata": {
        "id": "8b898acb"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5cb5348",
      "metadata": {
        "id": "e5cb5348"
      },
      "outputs": [],
      "source": [
        "# Apply RF with resource measurements\n",
        "def apply_rf(X_train, y_train, best_params=None, random_state=42, n_jobs=-1, cv=5):\n",
        "    \"\"\"\n",
        "    Apply Random Forest with resource measurements, including memory, training time, and CPU usage.\n",
        "\n",
        "    Parameters:\n",
        "        X_train: Training features.\n",
        "        y_train: Training labels.\n",
        "        best_params: Dictionary of best parameters for Random Forest.\n",
        "        random_state: Random state for reproducibility.\n",
        "        n_jobs: Number of jobs for parallel processing.\n",
        "        cv: Number of cross-validation folds.\n",
        "\n",
        "    Returns:\n",
        "        cv_scores_rf: Cross-validation scores.\n",
        "        measurement_rf: Dictionary of memory, training time, and CPU usage.\n",
        "        rf_model: Trained Random Forest model.\n",
        "    \"\"\"\n",
        "\n",
        "    measurement_rf = {}\n",
        "\n",
        "    # Default to empty dictionary if best_params is not provided\n",
        "    best_params = best_params or {}\n",
        "\n",
        "    rf_model = RandomForestClassifier(**best_params, random_state=random_state, n_jobs=n_jobs)\n",
        "\n",
        "    # Function to monitor CPU usage during training\n",
        "    cpu_usage = []\n",
        "    stop_flag = threading.Event()\n",
        "\n",
        "    def monitor_cpu():\n",
        "        while not stop_flag.is_set():\n",
        "            cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
        "\n",
        "    # Function to train the model\n",
        "    def train_model():\n",
        "        rf_model.fit(X_train, y_train)\n",
        "\n",
        "    try:\n",
        "        # Start CPU monitoring in a separate thread\n",
        "        cpu_thread = threading.Thread(target=monitor_cpu)\n",
        "        cpu_thread.start()\n",
        "\n",
        "        # Measure memory usage and training time\n",
        "        start_time = time.time()\n",
        "        train_memory_rf = max(memory_usage((train_model,)))  # Measure peak memory usage\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Stop CPU monitoring\n",
        "        stop_flag.set()\n",
        "        cpu_thread.join()\n",
        "\n",
        "        # Add measurements\n",
        "        measurement_rf['Memory Usage (MB)'] = train_memory_rf\n",
        "        measurement_rf['Training Time (s)'] = training_time\n",
        "        measurement_rf['Peak CPU Usage (%)'] = max(cpu_usage)\n",
        "        measurement_rf['Average CPU Usage (%)'] = sum(cpu_usage) / len(cpu_usage) if cpu_usage else 0\n",
        "\n",
        "        # Perform cross-validation\n",
        "        cv_scores_rf = cross_val_score(rf_model, X_train, y_train, cv=cv, n_jobs=n_jobs)\n",
        "\n",
        "        return cv_scores_rf, measurement_rf, rf_model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Random Forest training: {e}\")\n",
        "        return None, None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8f6d4fa",
      "metadata": {
        "id": "c8f6d4fa"
      },
      "outputs": [],
      "source": [
        "# Apply XGBoost with resource measurements\n",
        "def apply_xgboost(X_train, y_train, best_params=None, random_state=42, n_jobs=-1, cv=5):\n",
        "    \"\"\"\n",
        "    Apply XGBoost with resource measurements, including memory, training time, and CPU usage.\n",
        "\n",
        "    Parameters:\n",
        "        X_train: Training features.\n",
        "        y_train: Training labels.\n",
        "        best_params: Dictionary of best parameters for XGBoost.\n",
        "        random_state: Random state for reproducibility.\n",
        "        cv: Number of cross-validation folds.\n",
        "\n",
        "    Returns:\n",
        "        cv_scores_xgb: Cross-validation scores.\n",
        "        measurement_xgb: Dictionary of memory, training time, and CPU usage.\n",
        "        xgb_model: Trained XGBoost model.\n",
        "    \"\"\"\n",
        "\n",
        "    measurement_xgb = {}\n",
        "\n",
        "    # Default to empty dictionary if best_params is not provided\n",
        "    best_params = best_params or {}\n",
        "\n",
        "    xgb_model = xgb.XGBClassifier(**best_params, objective='multi:softmax', num_class=len(y_train.unique()), random_state=random_state, n_jobs=n_jobs)\n",
        "\n",
        "    # Function to monitor CPU usage during training\n",
        "    cpu_usage = []\n",
        "    stop_flag = threading.Event()\n",
        "\n",
        "    def monitor_cpu():\n",
        "        while not stop_flag.is_set():\n",
        "            cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
        "\n",
        "    # Function to train the model\n",
        "    def train_model():\n",
        "        xgb_model.fit(X_train, y_train)\n",
        "\n",
        "    try:\n",
        "        # Start CPU monitoring in a separate thread\n",
        "        cpu_thread = threading.Thread(target=monitor_cpu)\n",
        "        cpu_thread.start()\n",
        "\n",
        "        # Measure memory usage and training time\n",
        "        start_time = time.time()\n",
        "        train_memory_xgb = max(memory_usage((train_model,)))  # Measure peak memory usage\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Stop CPU monitoring\n",
        "        stop_flag.set()\n",
        "        cpu_thread.join()\n",
        "\n",
        "        # Add measurements\n",
        "        measurement_xgb['Memory Usage (MB)'] = train_memory_xgb\n",
        "        measurement_xgb['Training Time (s)'] = training_time\n",
        "        measurement_xgb['Peak CPU Usage (%)'] = max(cpu_usage)\n",
        "        measurement_xgb['Average CPU Usage (%)'] = sum(cpu_usage) / len(cpu_usage) if cpu_usage else 0\n",
        "\n",
        "        # Perform cross-validation\n",
        "        cv_scores_xgb = cross_val_score(xgb_model, X_train, y_train, cv=cv, n_jobs=n_jobs)\n",
        "\n",
        "        return cv_scores_xgb, measurement_xgb, xgb_model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during XGBoost training: {e}\")\n",
        "        return None, None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Add this NEW helper function before your existing apply_rf, apply_xgboost, apply_knn functions\n",
        "\n",
        "def cross_val_with_resampling(model, X, y, resampler=None, cv=5, n_jobs=-1):\n",
        "    \"\"\"\n",
        "    Perform cross-validation with resampling (SMOTE/undersampling) inside each fold.\n",
        "    This prevents data leakage from synthetic samples.\n",
        "\n",
        "    Parameters:\n",
        "        model: The classifier to evaluate\n",
        "        X: Training features\n",
        "        y: Training labels\n",
        "        resampler: Resampling strategy (SMOTE, RandomUnderSampler, etc.). If None, no resampling.\n",
        "        cv: Number of cross-validation folds\n",
        "        n_jobs: Number of parallel jobs\n",
        "\n",
        "    Returns:\n",
        "        cv_scores: Array of cross-validation scores\n",
        "    \"\"\"\n",
        "    if resampler is not None:\n",
        "        # Create pipeline that applies resampling INSIDE each CV fold\n",
        "        pipeline = ImbPipeline([\n",
        "            ('resampler', resampler),\n",
        "            ('classifier', model)\n",
        "        ])\n",
        "    else:\n",
        "        pipeline = model\n",
        "\n",
        "    # Perform cross-validation\n",
        "    cv_scores = cross_val_score(pipeline, X, y, cv=cv, n_jobs=n_jobs, scoring='accuracy')\n",
        "\n",
        "    return cv_scores\n",
        "\n",
        "\n",
        "# MODIFIED apply_rf function\n",
        "def apply_rf(X_train, y_train, best_params=None, random_state=42, n_jobs=-1, cv=5,\n",
        "             use_resampling=False, resampler=None):\n",
        "    \"\"\"\n",
        "    Apply Random Forest with resource measurements, including memory, training time, and CPU usage.\n",
        "\n",
        "    NEW PARAMETER:\n",
        "        use_resampling: If True, apply resampling inside CV folds (prevents leakage)\n",
        "        resampler: The resampling strategy to use (e.g., SMOTE, RandomUnderSampler)\n",
        "    \"\"\"\n",
        "\n",
        "    measurement_rf = {}\n",
        "    best_params = best_params or {}\n",
        "\n",
        "    rf_model = RandomForestClassifier(**best_params, random_state=random_state, n_jobs=n_jobs)\n",
        "\n",
        "    # Function to monitor CPU usage during training\n",
        "    cpu_usage = []\n",
        "    stop_flag = threading.Event()\n",
        "\n",
        "    def monitor_cpu():\n",
        "        while not stop_flag.is_set():\n",
        "            cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
        "\n",
        "    # Function to train the model\n",
        "    def train_model():\n",
        "        rf_model.fit(X_train, y_train)\n",
        "\n",
        "    try:\n",
        "        # Start CPU monitoring in a separate thread\n",
        "        cpu_thread = threading.Thread(target=monitor_cpu)\n",
        "        cpu_thread.start()\n",
        "\n",
        "        # Measure memory usage and training time\n",
        "        start_time = time.time()\n",
        "        train_memory_rf = max(memory_usage((train_model,)))\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Stop CPU monitoring\n",
        "        stop_flag.set()\n",
        "        cpu_thread.join()\n",
        "\n",
        "        # Add measurements\n",
        "        measurement_rf['Memory Usage (MB)'] = train_memory_rf\n",
        "        measurement_rf['Training Time (s)'] = training_time\n",
        "        measurement_rf['Peak CPU Usage (%)'] = max(cpu_usage) if cpu_usage else 0\n",
        "        measurement_rf['Average CPU Usage (%)'] = sum(cpu_usage) / len(cpu_usage) if cpu_usage else 0\n",
        "\n",
        "        # Perform cross-validation with proper resampling\n",
        "        if use_resampling and resampler is not None:\n",
        "            # Use the new function that applies resampling inside CV\n",
        "            cv_scores_rf = cross_val_with_resampling(\n",
        "                RandomForestClassifier(**best_params, random_state=random_state, n_jobs=n_jobs),\n",
        "                X_train,\n",
        "                y_train,\n",
        "                resampler=resampler,\n",
        "                cv=cv,\n",
        "                n_jobs=n_jobs\n",
        "            )\n",
        "        else:\n",
        "            # Standard CV without resampling\n",
        "            cv_scores_rf = cross_val_score(rf_model, X_train, y_train, cv=cv, n_jobs=n_jobs)\n",
        "\n",
        "        return cv_scores_rf, measurement_rf, rf_model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Random Forest training: {e}\")\n",
        "        return None, None, None"
      ],
      "metadata": {
        "id": "rux19S8_DNUw"
      },
      "id": "rux19S8_DNUw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b305468",
      "metadata": {
        "id": "0b305468"
      },
      "outputs": [],
      "source": [
        "# Apply KNN with resource measurements\n",
        "def apply_knn(X_train, y_train, best_params=None, n_jobs=-1, cv=5):\n",
        "    \"\"\"\n",
        "    Apply K-Nearest Neighbors (KNN) with resource measurements, including memory, training time, and CPU usage.\n",
        "\n",
        "    Parameters:\n",
        "        X_train: Training features.\n",
        "        y_train: Training labels.\n",
        "        best_params: Dictionary of best parameters for KNN.\n",
        "        cv: Number of cross-validation folds.\n",
        "\n",
        "    Returns:\n",
        "        cv_scores_knn: Cross-validation scores.\n",
        "        measurement_knn: Dictionary of memory, training time, and CPU usage.\n",
        "        knn_model: Trained KNN model.\n",
        "    \"\"\"\n",
        "\n",
        "    measurement_knn = {}\n",
        "\n",
        "    # Default to empty dictionary if best_params is not provided\n",
        "    best_params = best_params or {}\n",
        "\n",
        "    # Initialize KNN model\n",
        "    knn_model = KNeighborsClassifier(**best_params, n_jobs=n_jobs)\n",
        "\n",
        "    # Function to monitor CPU usage during training\n",
        "    cpu_usage = []\n",
        "    stop_flag = threading.Event()\n",
        "\n",
        "    def monitor_cpu():\n",
        "        while not stop_flag.is_set():\n",
        "            cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
        "\n",
        "    # Function to train the model\n",
        "    def train_model():\n",
        "        knn_model.fit(X_train, y_train)\n",
        "\n",
        "    try:\n",
        "        # Start CPU monitoring in a separate thread\n",
        "        cpu_thread = threading.Thread(target=monitor_cpu)\n",
        "        cpu_thread.start()\n",
        "\n",
        "        # Measure memory usage and training time\n",
        "        start_time = time.time()\n",
        "        train_memory_knn = max(memory_usage((train_model,)))  # Measure peak memory usage\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Stop CPU monitoring\n",
        "        stop_flag.set()\n",
        "        cpu_thread.join()\n",
        "\n",
        "        # Add measurements\n",
        "        measurement_knn['Memory Usage (MB)'] = train_memory_knn\n",
        "        measurement_knn['Training Time (s)'] = training_time\n",
        "        measurement_knn['Peak CPU Usage (%)'] = max(cpu_usage)\n",
        "        measurement_knn['Average CPU Usage (%)'] = sum(cpu_usage) / len(cpu_usage) if cpu_usage else 0\n",
        "\n",
        "        # Perform cross-validation\n",
        "        cv_scores_knn = cross_val_score(knn_model, X_train, y_train, cv=cv, n_jobs=n_jobs)\n",
        "\n",
        "        return cv_scores_knn, measurement_knn, knn_model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during KNN training: {e}\")\n",
        "        return None, None, None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40b00b5b",
      "metadata": {
        "id": "40b00b5b"
      },
      "source": [
        "# 1. Loading and Preparing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9efd8d37",
      "metadata": {
        "id": "9efd8d37"
      },
      "outputs": [],
      "source": [
        "# Loading the dataset\n",
        "clean_df = pd.read_csv('/content/drive/MyDrive/Data/cicids2017_cleaned.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== TEMPORAL SPLIT FIX =====\n",
        "# Check if your dataset has a timestamp or date column\n",
        "# If you have a timestamp column (e.g., 'Timestamp'), use this:\n",
        "\n",
        "# Option 1: If you have a timestamp column\n",
        "if 'Timestamp' in clean_df.columns:\n",
        "    # Sort by timestamp to ensure chronological order\n",
        "    clean_df = clean_df.sort_values('Timestamp').reset_index(drop=True)\n",
        "\n",
        "    # Calculate split index (70% train, 30% test)\n",
        "    split_idx = int(len(clean_df) * 0.7)\n",
        "\n",
        "    # Split chronologically\n",
        "    train_df = clean_df.iloc[:split_idx]\n",
        "    test_df = clean_df.iloc[split_idx:]\n",
        "\n",
        "    # Separate features and target\n",
        "    X_train = train_df.drop(['Attack Type', 'Timestamp'], axis=1)  # Drop timestamp from features\n",
        "    y_train = train_df['Attack Type']\n",
        "    X_test = test_df.drop(['Attack Type', 'Timestamp'], axis=1)\n",
        "    y_test = test_df['Attack Type']\n",
        "\n",
        "    print(f\"Train set: {len(train_df)} samples\")\n",
        "    print(f\"Test set: {len(test_df)} samples\")\n",
        "    print(f\"\\nTrain date range: {train_df['Timestamp'].min()} to {train_df['Timestamp'].max()}\")\n",
        "    print(f\"Test date range: {test_df['Timestamp'].min()} to {test_df['Timestamp'].max()}\")\n",
        "\n",
        "# Option 2: If you DON'T have a timestamp (but data is already chronologically ordered)\n",
        "else:\n",
        "    print(\"WARNING: No timestamp column found. Assuming data is already in chronological order.\")\n",
        "    print(\"If data was shuffled during preprocessing, this will cause data leakage!\")\n",
        "\n",
        "    # Preparing features and target\n",
        "    X = clean_df.drop('Attack Type', axis=1)\n",
        "    y = clean_df['Attack Type']\n",
        "\n",
        "    # Calculate split index (70% train, 30% test)\n",
        "    split_idx = int(len(clean_df) * 0.7)\n",
        "\n",
        "    # Split chronologically (assumes data is already sorted by time)\n",
        "    X_train = X.iloc[:split_idx]\n",
        "    y_train = y.iloc[:split_idx]\n",
        "    X_test = X.iloc[split_idx:]\n",
        "    y_test = y.iloc[split_idx:]\n",
        "\n",
        "    print(f\"Train set: {len(X_train)} samples\")\n",
        "    print(f\"Test set: {len(X_test)} samples\")\n",
        "\n",
        "# Check class distribution after temporal split\n",
        "print(\"\\nTraining set class distribution:\")\n",
        "print(y_train.value_counts())\n",
        "print(\"\\nTest set class distribution:\")\n",
        "print(y_test.value_counts())"
      ],
      "metadata": {
        "id": "9vkAhvZfDFx7"
      },
      "id": "9vkAhvZfDFx7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "057c62f8",
      "metadata": {
        "id": "057c62f8"
      },
      "outputs": [],
      "source": [
        "clean_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d88fe01c",
      "metadata": {
        "id": "d88fe01c"
      },
      "source": [
        "## 1.1. Preparing Training and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bee82dd5",
      "metadata": {
        "id": "bee82dd5"
      },
      "outputs": [],
      "source": [
        "# Preparing training and test splits\n",
        "X = clean_df.drop('Attack Type', axis=1)\n",
        "y = clean_df['Attack Type']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d17c5f2",
      "metadata": {
        "id": "0d17c5f2"
      },
      "source": [
        "### 1.1.1. Feature Scaling\n",
        "\n",
        "Feature scaling is essential for ensuring that all features contribute equally to the model's performance, improving convergence speed, and enhancing the accuracy of machine learning algorithms, particularly those that rely on distance metrics. The first step is to decide on the appropriate type of scaling. To do this, examining outliers and the distribution of features is an effective approach."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3467587b",
      "metadata": {
        "id": "3467587b"
      },
      "source": [
        "* StandardScaler is best for normally distributed data.\n",
        "* MinMaxScaler is best for bounded features.\n",
        "* Robust Scaler is best when outliers are present, ensuring that the scaling maintains the robustness of the data.\n",
        "\n",
        "\n",
        "| **Aspect**             | **StandardScaler**                          | **MinMaxScaler**                          | **RobustScaler**                          |  \n",
        "|-----------------------|--------------------------------------------|------------------------------------------|-------------------------------------------|  \n",
        "| **Scaling Approach**  | Mean and standard deviation                 | Minimum and maximum                      | Median and interquartile range            |  \n",
        "| **Sensitivity to Outliers** | Sensitive (affected by outliers)       | Sensitive (max and min can distort)     | Not sensitive (robust to outliers)        |  \n",
        "| **Range After Scaling** | Normal distribution (mean=0, sd=1)      | Custom range (default [0,1])            | No fixed range (median is used)           |  \n",
        "| **Best For**          | Normal distributions                        | Bounded data                             | Data with outliers or non-Gaussian types  |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "077b5c09",
      "metadata": {
        "id": "077b5c09"
      },
      "source": [
        "Based on the [exploratory data analysis (EDA)](https://www.kaggle.com/code/ericanacletoribeiro/cicids2017-comprehensive-data-processing-for-ml), most features exhibit a high percentage of outliers, with values exceeding 10%. As a result, Robust Scaling is likely the most appropriate preprocessing technique. Additionally, statistical tests, such as the Anderson-Darling test (also available on the beforementioned Notebook), indicated that the null hypothesis was rejected for all of the features, confirming that the data does not follow a normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "069f4070",
      "metadata": {
        "id": "069f4070"
      },
      "outputs": [],
      "source": [
        "# Initialize RobustScaler\n",
        "scaler = RobustScaler()\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform the test set using the fitted scaler\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1550853f",
      "metadata": {
        "id": "1550853f"
      },
      "source": [
        "| **Model**                  | **Is Scaling Important?** | **Explanation**                                                                 |\n",
        "|----------------------------|---------------------------|---------------------------------------------------------------------------------|\n",
        "| **Random Forest**          | No                       | Random Forest is tree-based and works with the raw values without being affected by scaling. |\n",
        "| **XGBoost**                | No                       | Like Random Forest, XGBoost is tree-based and insensitive to feature scaling.   |\n",
        "| **KNN**    | Yes                      | KNN uses distance metrics, which are affected by the scale of features. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd78e7b",
      "metadata": {
        "id": "0bd78e7b"
      },
      "outputs": [],
      "source": [
        "# Exporting the scaler in case it is needed during deployment\n",
        "joblib.dump(scaler, 'robust_scaler.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5facf166",
      "metadata": {
        "id": "5facf166"
      },
      "source": [
        "### 1.1.2. Resampling Techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53145437",
      "metadata": {
        "id": "53145437"
      },
      "outputs": [],
      "source": [
        "# Checking the distribution of the target variable\n",
        "clean_df['Attack Type'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e4c56f7",
      "metadata": {
        "id": "3e4c56f7"
      },
      "source": [
        "The dataset shows a considerable imbalance, as already discussed on the [previous notebook](https://www.kaggle.com/datasets/ericanacletoribeiro/cicids2017-cleaned-and-preprocessed). Since cross-validation results are already available without addressing this issue (serving as a benchmark), the decision here is to undersample the majority class to reduce complexity while aiming to maintain performance. This can help the model pay more attention to the less frequent attack types, which are often the focus in intrusion detection systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc49e4da",
      "metadata": {
        "id": "bc49e4da"
      },
      "outputs": [],
      "source": [
        "# Initializing the undersampling for the clean df\n",
        "X_train_resampled, y_train_resampled = RandomUnderSampler(sampling_strategy={'Normal Traffic': 500000}, random_state=42).fit_resample(X_train, y_train)\n",
        "\n",
        "# Initializing the undersampling for the scaled df\n",
        "X_train_scaled, y_train_scaled = RandomUnderSampler(sampling_strategy={'Normal Traffic': 500000}, random_state=42).fit_resample(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae893b72",
      "metadata": {
        "id": "ae893b72"
      },
      "source": [
        "In order to further balance the training set, we can also partially oversample the minority classes with SMOTE. _Synthetic Minority Over-sampling Technique_ is a widely-used method to address class imbalance in datasets by generating synthetic samples for the minority class, rather than merely duplicating existing ones. The values were chosen based on the value_counts above: all in all, rouding up the numbers in order to avoid an overly aggressive oversampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aab8f70",
      "metadata": {
        "id": "2aab8f70"
      },
      "outputs": [],
      "source": [
        "# Initializing the oversampling for the scaled df\n",
        "X_train_resampled_scaled, y_train_resampled_scaled = SMOTE(sampling_strategy={'Bots': 2000, 'Web Attacks': 2000, 'Brute Force': 7000, 'Port Scanning': 70000, 'DDoS':90000, 'DoS': 200000}, random_state=42).fit_resample(X_train_scaled, y_train_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "063034e8",
      "metadata": {
        "id": "063034e8"
      },
      "source": [
        "Since SMOTE relies on distance calculations to generate synthetic samples, it is essential to scale the dataset beforehand to ensure that all features contribute equally to the interpolation process and avoid skewed or meaningless synthetic data. Therefore, this process will not be applied to the raw training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8b85c9e",
      "metadata": {
        "id": "b8b85c9e"
      },
      "outputs": [],
      "source": [
        "# Cleaning up\n",
        "del X_train_scaled, X_train, y_train, X, y, clean_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8853628",
      "metadata": {
        "id": "f8853628"
      },
      "outputs": [],
      "source": [
        "# Checking the distribution of the attack types in the resampled/raw training set\n",
        "y_train_scaled.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "880c965b",
      "metadata": {
        "id": "880c965b"
      },
      "outputs": [],
      "source": [
        "# Checking the distribution of the attack types in the resampled/scaled training set\n",
        "y_train_resampled_scaled.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4d95e46",
      "metadata": {
        "id": "a4d95e46"
      },
      "source": [
        "### 1.1.3. A Note on PCA\n",
        "\n",
        "During the experimentation process, PCA (Principal Component Analysis) was considered as a dimensionality reduction technique to speed up model training and improve computational efficiency. While PCA proved effective in accelerating training for algorithms like KNN, it introduced an additional preprocessing step in the pipeline, which is not ideal for real-time production environments. Production systems prioritize simplicity and speed, and the need to transform new data using PCA before making predictions could potentially impact real-time performance.\n",
        "\n",
        "Moreover, the dataset exhibited strong correlations among features, causing PCA to converge the variance into just one or two principal components. While this initially seemed promising due to the high explained variance ratio, these components failed to fully capture the intricacies of the data. This led to poorer results in terms of model performance, as the reduced representation lacked the necessary granularity to differentiate between classes effectively. Based on these observations, PCA was ultimately excluded from the workflow to maintain a balance between computational efficiency, simplicity, and model performance on production."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df594bed",
      "metadata": {
        "id": "df594bed"
      },
      "source": [
        "# 2. Supervised Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4000fa72",
      "metadata": {
        "id": "4000fa72"
      },
      "source": [
        "## 2.1. Random Forest\n",
        "\n",
        "Random Forest is robust to variance in the data. Therefore, it can be trained on the non-scaled version of the dataset without a significant impact on performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3154fd69",
      "metadata": {
        "id": "3154fd69"
      },
      "source": [
        "### 2.1.1. Hyperparameter Tuning*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4b10468",
      "metadata": {
        "id": "c4b10468"
      },
      "source": [
        "To determine the optimal parameters for machine learning models, it's crucial to evaluate how different hyperparameters affect performance. A good way to do this is by visualizing the model's performance across various configurations, using techniques such as RandomizedSearchCV or GridSearchCV.\n",
        "\n",
        "While an exhaustive grid search is often recommended for thorough hyperparameter optimization through cross-validation (CV), this process can be computationally expensive and time-consuming. Given the performance constraints of this project, the goal here is to simplify the search by using RandomizedSearchCV. This approach randomly samples parameter combinations and checks if they can outperform the standard parameter settings of each model.\n",
        "\n",
        "***Due to resources limitations on Kaggle, this section was run locally, results are shown at the end**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fed0a56f",
      "metadata": {
        "id": "fed0a56f"
      },
      "outputs": [],
      "source": [
        " # Defining the parameters for the Random Forest Classifier\n",
        " #param_grid = {\n",
        "#     'n_estimators': [100, 150, 200],\n",
        "#     'max_depth': [20, 30, None],\n",
        "#     'min_samples_split': [2, 5, 10],\n",
        "#     'min_samples_leaf': [1, 2, 4],\n",
        "#     'max_features': ['sqrt', 'log2'],\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c766354",
      "metadata": {
        "id": "3c766354",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Creating the Random Forest Classifier\n",
        "#rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d68a9f89",
      "metadata": {
        "id": "d68a9f89"
      },
      "outputs": [],
      "source": [
        "# # Saving results with the standard parameters\n",
        "#cv_sc_rf = cross_val_score(rf_model, X_train_resampled, y_train_resampled, cv=3, n_jobs=-1)\n",
        "#cv_sc_rf = np.mean(cv_sc_rf)\n",
        "\n",
        " # Apply RandomizedSearchCV\n",
        "#random_search_rf = RandomizedSearchCV(estimator=rf_model, param_distributions=param_grid, n_iter=20, cv=3, n_jobs=-1, verbose=2)\n",
        "#random_search_rf.fit(X_train_resampled, y_train_resampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d0b7cba",
      "metadata": {
        "id": "4d0b7cba"
      },
      "outputs": [],
      "source": [
        " # Get the best parameters\n",
        " #print(f'Best Parameters: {random_search_rf.best_params_}')\n",
        " #print(f\"Best Cross-Validation Score: {random_search_rf.best_score_}\")\n",
        " #print(f\"Cross-Validation from Standard: {cv_sc_rf}\")\n",
        "\n",
        " #best_params_rf = random_search_rf.best_params_ if random_search_rf.best_score_ > cv_sc_rf else None\n",
        "\n",
        "# del random_search_rf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b090c7a",
      "metadata": {
        "id": "0b090c7a"
      },
      "source": [
        "**A Note on Hyperparameter Tuning**\n",
        "\n",
        "Due to the stochastic nature of RandomizedSearchCV, the values for each parameter might differ every time this notebook is run. To ensure reproducibility, the following parameters were selected as the best-performing ones for the Random Forest model in this experiment:\n",
        "\n",
        "Best Parameters:\n",
        "- `n_estimators`: 200\n",
        "- `min_samples_split`: 5\n",
        "- `min_samples_leaf`: 2\n",
        "- `max_features`: 'sqrt'\n",
        "- `max_depth`: None\n",
        "\n",
        "If you wish to replicate my results, these are the parameters that provided the optimal performance for this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ee24767",
      "metadata": {
        "id": "0ee24767"
      },
      "outputs": [],
      "source": [
        "# Defining manually due to Kaggle performance limitations\n",
        "best_params_rf = {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d206c3b7",
      "metadata": {
        "id": "d206c3b7"
      },
      "source": [
        "### 2.1.2. Fitting the Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29e079f7",
      "metadata": {
        "id": "29e079f7"
      },
      "outputs": [],
      "source": [
        "# Fitting the model\n",
        "cv_scores_rf, measurement_rf, rf_model = apply_rf(X_train_resampled, y_train_resampled, best_params=best_params_rf)\n",
        "\n",
        "# Making predictions\n",
        "y_pred_rf = rf_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a96653d2",
      "metadata": {
        "id": "a96653d2"
      },
      "outputs": [],
      "source": [
        "# Evaluating the model performance on the cross validation set vs accuracy on the test set\n",
        "cv_scores_mean_rf = np.mean(cv_scores_rf)\n",
        "print(f'Cross validation average score: {cv_scores_mean_rf:.4f} +/- standard deviation: {np.std(cv_scores_rf):.4f}')\n",
        "\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f'Accuracy on the test set: {accuracy_rf:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20989149",
      "metadata": {
        "id": "20989149"
      },
      "outputs": [],
      "source": [
        "# Checking computational cost\n",
        "print(\"Resource measurements:\", measurement_rf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11ff7626",
      "metadata": {
        "id": "11ff7626"
      },
      "outputs": [],
      "source": [
        "# Evaluating the model via confusion matrix\n",
        "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', xticklabels=rf_model.classes_, yticklabels=rf_model.classes_, cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')\n",
        "plt.title('Random ForestConfusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5a84109",
      "metadata": {
        "id": "c5a84109"
      },
      "outputs": [],
      "source": [
        "# Classification report\n",
        "print(classification_report(y_test, y_pred_rf))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaa62b3e",
      "metadata": {
        "id": "aaa62b3e"
      },
      "source": [
        "With a mean cross-validation score of 0.9987 and an accuracy score of 0.9988, the model demonstrates strong performance without signs of underfitting or overfitting (your numbers might differ due to different results from RamdomizedSearchCV). Additionally, the confusion matrix reveals a significant improvement in recall (from 0.74 to 0.83) for the 'Bots' class compared to the benchmark model ([available here](https://www.kaggle.com/code/ericanacletoribeiro/cicids2017-comprehensive-data-processing-for-ml)).\n",
        "While the precision score for the same class decreased from 0.86 to 0.71, the trade-off is justified in the context of a NIDS. In such systems, minimizing false negatives is crucial, as undetected threats can compromise network security. False positives, while inconvenient, are less harmful than false negatives. This improvement in recall indicates that the model is now better at detecting 'Bots' attacks, validating the effectiveness of fine-tuning techniques for this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebc3b622",
      "metadata": {
        "id": "ebc3b622"
      },
      "source": [
        "### 2.1.3. Exporting the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54d9ca01",
      "metadata": {
        "id": "54d9ca01"
      },
      "outputs": [],
      "source": [
        "# Save the model to a file\n",
        "joblib.dump(rf_model, 'random_forest.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61a5d292",
      "metadata": {
        "id": "61a5d292"
      },
      "source": [
        "## 2.2. XGBoost\n",
        "\n",
        "XGBoost (Extreme Gradient Boosting) is a highly efficient implementation of gradient boosting, which is known for its power in handling imbalanced datasets, preventing overfitting, and its ability to work well with both structured and unstructured data. In this section, we will evaluate XGBoost for the task of anomaly-based intrusion detection.\n",
        "\n",
        "While Random Forest is an ensemble method that builds multiple decision trees independently, XGBoost takes a more iterative approach, building trees sequentially, where each new tree attempts to correct errors made by the previous one. This results in a model that often outperforms Random Forest in terms of predictive accuracy, particularly for complex datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bb77869",
      "metadata": {
        "id": "4bb77869"
      },
      "source": [
        "### 2.2.1. Hyperparameter Tuning*\n",
        "\n",
        "***Due to resources limitations on Kaggle, this section was run locally, results are showns at the end**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d426b963",
      "metadata": {
        "id": "d426b963"
      },
      "outputs": [],
      "source": [
        "# # Defining the parameter grid for XGBoost\n",
        "# param_dist = {\n",
        "#     'n_estimators': [100, 150, 200],\n",
        "#     'max_depth': [3, 6, 9],\n",
        "#     'learning_rate': [0.2, 0.3, 0.4],\n",
        "#     'subsample': [0.7, 0.8, 1.0],\n",
        "#     'colsample_bytree': [0.7, 0.8, 1.0],\n",
        "#     'min_child_weight': [1, 5, 10],\n",
        "# }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a14b4b0",
      "metadata": {
        "id": "1a14b4b0"
      },
      "source": [
        "XGBoost's `multi:softmax` objective requires numerical labels for classification. Therefore, a mapping is necessary to convert categorical labels into numerical values before training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a49983f7",
      "metadata": {
        "id": "a49983f7"
      },
      "outputs": [],
      "source": [
        "# # Creating the XGBoost Classifier\n",
        "# xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(y_train_resampled.unique()), random_state=42, n_jobs=-1)\n",
        "\n",
        "# Custom mapping for the attack types\n",
        "label_mapping = {\n",
        "    'Normal Traffic': 0,\n",
        "    'DoS': 1,\n",
        "    'DDoS': 2,\n",
        "    'Port Scanning': 3,\n",
        "    'Brute Force': 4,\n",
        "    'Web Attacks': 5,\n",
        "    'Bots': 6\n",
        "}\n",
        "y_train_resampled_mapped = y_train_resampled.map(label_mapping)\n",
        "y_test_mapped = y_test.map(label_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e7c4a0c",
      "metadata": {
        "id": "3e7c4a0c"
      },
      "outputs": [],
      "source": [
        " # Saving results with the standard parameters\n",
        " #cv_sc_xgb = cross_val_score(xgb_model, X_train_resampled, y_train_resampled_mapped, cv=3, n_jobs=-1)\n",
        " #cv_sc_xgb = np.mean(cv_sc_xgb)\n",
        "\n",
        " # Perform RandomizedSearchCV\n",
        "# random_search_xgb = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_dist, n_iter=30, cv=3, n_jobs=-1, verbose=2, random_state=42)\n",
        "# random_search_xgb.fit(X_train_resampled, y_train_resampled_mapped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "157939f0",
      "metadata": {
        "id": "157939f0"
      },
      "outputs": [],
      "source": [
        " # Best parameters found by RandomizedSearchCV\n",
        " #print(f'Best Parameters for XGBoost: {random_search_xgb.best_params_}')\n",
        " #print(f\"Best Cross-Validation Score: {random_search_xgb.best_score_}\")\n",
        "# print(f\"Cross-Validation from Standard: {cv_sc_xgb}\")\n",
        "\n",
        "# best_params_xgb = random_search_xgb.best_params_ if random_search_xgb.best_score_ > cv_sc_xgb else None\n",
        "\n",
        "# del random_search_xgb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a484059b",
      "metadata": {
        "id": "a484059b"
      },
      "source": [
        "**A Note on Hyperparameter Tuning**\n",
        "\n",
        "Due to the stochastic nature of RandomizedSearchCV, the values for each parameter might differ every time this notebook is run. To ensure reproducibility, the following parameters were selected as the best-performing ones for the XGBoost model in this experiment:\n",
        "\n",
        "Best Parameters:\n",
        "- `subsample`: 1.0\n",
        "- `n_estimators`: 150\n",
        "- `min_child_weight`: 1\n",
        "- `max_depth`: 3\n",
        "- `learning_rate`: 0.3\n",
        "- `colsample_bytree`: 0.7\n",
        "\n",
        "If you wish to replicate my results, these are the parameters that provided the optimal performance for this model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3c75e79",
      "metadata": {
        "id": "f3c75e79"
      },
      "outputs": [],
      "source": [
        "# Defining parameters manually due to Kaggles performance limitataions\n",
        "best_params_xgb = {'subsample': 1.0, 'n_estimators': 150, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.3, 'colsample_bytree': 0.7}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbf9f092",
      "metadata": {
        "id": "dbf9f092"
      },
      "source": [
        "### 2.2.2. Fitting the XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b709be8",
      "metadata": {
        "id": "6b709be8"
      },
      "outputs": [],
      "source": [
        "# Fitting the model\n",
        "cv_scores_xgb, measurement_xgb, xgb_model = apply_xgboost(X_train_resampled, y_train_resampled_mapped, best_params=best_params_xgb)\n",
        "\n",
        "# Making predictions\n",
        "y_pred_xgb = xgb_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f5b3864",
      "metadata": {
        "id": "7f5b3864"
      },
      "outputs": [],
      "source": [
        "# Evaluating the model performance on the cross validation set vs accuracy on the test set\n",
        "cv_scores_mean_xgb = np.mean(cv_scores_xgb)\n",
        "print(f'Cross validation average score: {cv_scores_mean_xgb:.4f} +/- standard deviation: {np.std(cv_scores_xgb):.4f}')\n",
        "\n",
        "accuracy_xgb = accuracy_score(y_test_mapped, y_pred_xgb)\n",
        "print(f'Accuracy on the test set: {accuracy_xgb:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fa67072",
      "metadata": {
        "id": "3fa67072"
      },
      "outputs": [],
      "source": [
        "# Checking computational cost\n",
        "print(\"Resource measurements:\", measurement_xgb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91b92f54",
      "metadata": {
        "id": "91b92f54"
      },
      "outputs": [],
      "source": [
        "# Remapping the labels for visualization\n",
        "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
        "predicted_labels = [reverse_label_mapping[pred] for pred in y_pred_xgb]\n",
        "actual_labels = sorted([reverse_label_mapping[label] for label in xgb_model.classes_])\n",
        "\n",
        "# Confusion matrix\n",
        "cm_xgb = confusion_matrix(y_test, predicted_labels)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm_xgb, annot=True, fmt='d', xticklabels=actual_labels, yticklabels=actual_labels, cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')\n",
        "plt.title('XGBoost Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a84452c",
      "metadata": {
        "id": "1a84452c"
      },
      "outputs": [],
      "source": [
        "# Classification report\n",
        "print(classification_report(y_test, predicted_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5adddc2",
      "metadata": {
        "id": "a5adddc2"
      },
      "source": [
        "The XGBoost model demonstrated a significant improvement in recall for the 'Bots' class compared to the Random Forest model. Recall is a critical metric for this application, as undetected attacks can compromise network security. The difference between the two models across other metrics, including accuracy, precision, and overall F1-score, remains negligible, with a marginal advantage to XGB."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5f37c47",
      "metadata": {
        "id": "e5f37c47"
      },
      "source": [
        "### 2.2.3. Exporting the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67849dd6",
      "metadata": {
        "id": "67849dd6"
      },
      "outputs": [],
      "source": [
        "# Save the model to a file\n",
        "joblib.dump(xgb_model, 'xgboost.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9a3fd06",
      "metadata": {
        "id": "f9a3fd06"
      },
      "source": [
        "## 2.3. K-Nearest Neighbors (KNN)\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a simple and effective classification algorithm that works by finding the majority class among the `k` nearest data points for a given instance. It is sensitive to the scale of the data, so it is important to ensure that the features are standardized (scaled)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0647ea6",
      "metadata": {
        "id": "d0647ea6"
      },
      "source": [
        "### 2.3.1. Hyperparamenter Tuning*\n",
        "\n",
        "***Due to resources limitations on Kaggle, this section was run locally, results are showns at the end**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f7ac68f",
      "metadata": {
        "id": "6f7ac68f"
      },
      "outputs": [],
      "source": [
        "# # Defining the parameters for KNN\n",
        "#param_grid_knn = {\n",
        "#     'n_neighbors': [3, 5, 7],\n",
        "#     'weights': ['uniform', 'distance'],\n",
        " #}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2009a43d",
      "metadata": {
        "id": "2009a43d"
      },
      "outputs": [],
      "source": [
        " # Creating the KNN model\n",
        " #knn_model = KNeighborsClassifier(n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f2cc492",
      "metadata": {
        "id": "1f2cc492"
      },
      "outputs": [],
      "source": [
        " # Saving results with the standard parameters\n",
        " #cv_sc_knn = cross_val_score(knn_model, X_train_resampled_scaled, y_train_resampled_scaled, cv=3, n_jobs=-1)\n",
        " #cv_sc_knn = np.mean(cv_sc_knn)\n",
        "\n",
        " # Apply RandomizedSearchCV\n",
        " #random_search_knn = RandomizedSearchCV(estimator=knn_model, param_distributions=param_grid_knn, n_iter=6, cv=3, n_jobs=-1, verbose=2)\n",
        " #random_search_knn.fit(X_train_resampled_scaled, y_train_resampled_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "033832cb",
      "metadata": {
        "id": "033832cb"
      },
      "outputs": [],
      "source": [
        " # Get the best parameters\n",
        "# print(f'Best Parameters: {random_search_knn.best_params_}')\n",
        "# print(f\"Best Cross-Validation Score: {random_search_knn.best_score_}\")\n",
        " #print(f\"Cross-Validation from Standard: {cv_sc_knn}\")\n",
        "\n",
        " #best_params_knn = random_search_knn.best_params_ if random_search_knn.best_score_ > cv_sc_knn else None\n",
        "\n",
        "# del random_search_knn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "978f62d8",
      "metadata": {
        "id": "978f62d8"
      },
      "source": [
        "Best Parameters:\n",
        "\n",
        "- `weights`: distance\n",
        "- `n_neighbours`: 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a2191f1",
      "metadata": {
        "id": "0a2191f1"
      },
      "outputs": [],
      "source": [
        "# Defining manually\n",
        "best_params_knn = {'weights': 'distance', 'n_neighbors': 3}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fbb0dbd",
      "metadata": {
        "id": "5fbb0dbd"
      },
      "source": [
        "### 2.3.2. Fitting the KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6003e1f5",
      "metadata": {
        "id": "6003e1f5"
      },
      "outputs": [],
      "source": [
        "# Fitting the model\n",
        "cv_scores_knn, measurement_knn, knn_model = apply_knn(X_train_resampled_scaled, y_train_resampled_scaled, best_params=best_params_knn)\n",
        "\n",
        "# Making predictions\n",
        "y_pred_knn = knn_model.predict(X_test_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea348497",
      "metadata": {
        "id": "ea348497"
      },
      "outputs": [],
      "source": [
        "# Evaluating the model performance on the cross-validation set vs accuracy on the test set\n",
        "cv_scores_mean_knn = np.mean(cv_scores_knn)\n",
        "print(f'Cross-validation average score: {cv_scores_mean_knn:.4f} +/- standard deviation: {np.std(cv_scores_knn):.4f}')\n",
        "\n",
        "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
        "print(f'Accuracy on the test set: {accuracy_knn:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69df40cc",
      "metadata": {
        "id": "69df40cc"
      },
      "outputs": [],
      "source": [
        "# Checking computational cost\n",
        "print(\"Resource measurements:\", measurement_knn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc54ead7",
      "metadata": {
        "id": "cc54ead7"
      },
      "outputs": [],
      "source": [
        "# Evaluating the model via confusion matrix\n",
        "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm_knn, annot=True, fmt='d', xticklabels=knn_model.classes_, yticklabels=knn_model.classes_, cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')\n",
        "plt.title('KNN Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "746308ea",
      "metadata": {
        "id": "746308ea"
      },
      "outputs": [],
      "source": [
        "# Classification report\n",
        "print(classification_report(y_test, y_pred_knn))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f219083f",
      "metadata": {
        "id": "f219083f"
      },
      "source": [
        "The KNN model also demonstrated strong performance, though it was slightly outperformed by the previous models. However, it is worth noting that the 'Bots' class remains the weakest in the dataset, with consistently lower values across all evaluation metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5564d1b",
      "metadata": {
        "id": "e5564d1b"
      },
      "source": [
        "### 2.1.3. Exporting the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7759e402",
      "metadata": {
        "id": "7759e402"
      },
      "outputs": [],
      "source": [
        "# Save the model to a file\n",
        "joblib.dump(knn_model, 'knn_model.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bfdb7d7",
      "metadata": {
        "id": "4bfdb7d7"
      },
      "source": [
        "# 3. Comparing Performance Results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Calculate ROC-AUC scores for each model\n",
        "y_pred_proba_rf = rf_model.predict_proba(X_test)\n",
        "roc_auc_rf = roc_auc_score(y_test, y_pred_proba_rf, multi_class='ovr', average='weighted')\n",
        "\n",
        "y_pred_proba_xgb = xgb_model.predict_proba(X_test)\n",
        "roc_auc_xgb = roc_auc_score(y_test_mapped, y_pred_proba_xgb, multi_class='ovr', average='weighted')\n",
        "\n",
        "y_pred_proba_knn = knn_model.predict_proba(X_test_scaled)\n",
        "roc_auc_knn = roc_auc_score(y_test, y_pred_proba_knn, multi_class='ovr', average='weighted')\n",
        "\n",
        "print(f'ROC-AUC Random Forest: {roc_auc_rf:.4f}')\n",
        "print(f'ROC-AUC XGBoost: {roc_auc_xgb:.4f}')\n",
        "print(f'ROC-AUC KNN: {roc_auc_knn:.4f}')\n"
      ],
      "metadata": {
        "id": "XaA5EPFQ2kIY"
      },
      "id": "XaA5EPFQ2kIY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff17041f",
      "metadata": {
        "id": "ff17041f"
      },
      "outputs": [],
      "source": [
        "# Calculating precision, recall, and F1 score for each model\n",
        "precision_rf = precision_score(y_test, y_pred_rf, average='weighted')\n",
        "recall_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
        "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
        "\n",
        "precision_xgb = precision_score(y_test_mapped, y_pred_xgb, average='weighted')\n",
        "recall_xgb = recall_score(y_test_mapped, y_pred_xgb, average='weighted')\n",
        "f1_xgb = f1_score(y_test_mapped, y_pred_xgb, average='weighted')\n",
        "\n",
        "precision_knn = precision_score(y_test, y_pred_knn, average='weighted')\n",
        "recall_knn = recall_score(y_test, y_pred_knn, average='weighted')\n",
        "f1_knn = f1_score(y_test, y_pred_knn, average='weighted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "040eb5f1",
      "metadata": {
        "id": "040eb5f1"
      },
      "outputs": [],
      "source": [
        "# Creating the results dataframe\n",
        "supervised_results = pd.DataFrame({\n",
        "    'Model': ['Random Forest', 'XGBoost', 'KNN'],\n",
        "    'Accuracy': [accuracy_rf, accuracy_xgb, accuracy_knn],\n",
        "    'Cross Validation Mean': [cv_scores_mean_rf, cv_scores_mean_xgb, cv_scores_mean_knn],\n",
        "    'Precision': [precision_rf, precision_xgb, precision_knn],\n",
        "    'Recall': [recall_rf, recall_xgb, recall_knn],\n",
        "    'F1 Score': [f1_rf, f1_xgb, f1_knn],\n",
        "    'ROC-AUC': [roc_auc_rf, roc_auc_xgb, roc_auc_knn],  # ADD THIS LINE\n",
        "    'Memory Usage (MB)': [measurement_rf['Memory Usage (MB)'], measurement_xgb['Memory Usage (MB)'], measurement_knn['Memory Usage (MB)']],\n",
        "    'Training Time (s)': [measurement_rf['Training Time (s)'], measurement_xgb['Training Time (s)'], measurement_knn['Training Time (s)']],\n",
        "    'Peak CPU Usage (%)': [measurement_rf['Peak CPU Usage (%)'], measurement_xgb['Peak CPU Usage (%)'], measurement_knn['Peak CPU Usage (%)']],\n",
        "    'Average CPU Usage (%)': [measurement_rf['Average CPU Usage (%)'], measurement_xgb['Average CPU Usage (%)'], measurement_knn['Average CPU Usage (%)']],\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24de1183",
      "metadata": {
        "id": "24de1183"
      },
      "outputs": [],
      "source": [
        "# Plotting the comparison for accuracy, cross-validation, and metrics\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Plotting Accuracy and Cross Validation Mean\n",
        "supervised_results.set_index('Model')[['Accuracy', 'Cross Validation Mean']].plot(kind='bar', ax=axes[0, 0], color=['skyblue', 'lightgreen'], legend=True)\n",
        "axes[0, 0].set_title('Model Comparison: Accuracy and Cross Validation Mean')\n",
        "axes[0, 0].set_ylabel('Score')\n",
        "axes[0, 0].set_xlabel('Model')\n",
        "axes[0, 0].set_ylim(0.95, 1.0)\n",
        "axes[0, 0].legend(loc='lower left')\n",
        "\n",
        "# Plotting Precision, Recall, F1 Score, and ROC-AUC (MODIFIED)\n",
        "supervised_results.set_index('Model')[['Precision', 'Recall', 'F1 Score', 'ROC-AUC']].plot(kind='bar', ax=axes[0, 1], color=['orange', 'lightcoral', 'yellowgreen', 'mediumpurple'], legend=True)\n",
        "axes[0, 1].set_title('Model Comparison: Precision, Recall, F1 Score, ROC-AUC')\n",
        "axes[0, 1].set_ylabel('Score')\n",
        "axes[0, 1].set_xlabel('Model')\n",
        "axes[0, 1].set_ylim(0.95, 1.0)\n",
        "axes[0, 1].legend(loc='lower left')\n",
        "\n",
        "# Plotting Memory Usage and Training Time\n",
        "ax1 = axes[1, 0]\n",
        "\n",
        "supervised_results.set_index('Model')['Memory Usage (MB)'].plot(\n",
        "    kind='bar', ax=ax1, color='lightblue', label='Memory Usage (MB)', width=0.6\n",
        ")\n",
        "ax1.set_ylabel('Memory Usage (MB)', color='lightblue')\n",
        "ax1.tick_params(axis='y', labelcolor='lightblue')\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "supervised_results.set_index('Model')['Training Time (s)'].plot(\n",
        "    ax=ax2, color='lightpink', marker='o', label='Training Time (s)'\n",
        ")\n",
        "ax2.set_ylabel('Training Time (s)', color='lightpink')\n",
        "ax2.tick_params(axis='y', labelcolor='lightpink')\n",
        "\n",
        "ax1.set_title('Model Comparison: Memory Usage and Training Time')\n",
        "ax1.set_xlabel('Model')\n",
        "\n",
        "lines, labels = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(lines + lines2, labels + labels2, loc='upper right')\n",
        "\n",
        "# Plotting Peak and Average CPU Usage\n",
        "supervised_results.set_index('Model')[['Peak CPU Usage (%)', 'Average CPU Usage (%)']].plot(kind='bar', ax=axes[1, 1], color=['lightgreen', 'salmon'], legend=True)\n",
        "axes[1, 1].set_title('Model Comparison: CPU Usage')\n",
        "axes[1, 1].set_ylabel('Percentage')\n",
        "axes[1, 1].set_xlabel('Model')\n",
        "axes[1, 1].legend(loc='lower left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f04f4ed3",
      "metadata": {
        "id": "f04f4ed3"
      },
      "source": [
        "[link text](https://)The plots effectively illustrate the performance of each model during training and testing phases. While Random Forest (RF) and XGBoost (XGB) achieved the highest accuracy and test scores, they also demanded more computational power. On the other hand, it's important to remember that KNN is a 'lazy learner,' meaning that actual training occurs when new data is introduced, either during testing or in production. The true performance of KNN will become clearer once the prototype is deployed and tested in real-time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GL9OwKaQ0w4V",
      "metadata": {
        "id": "GL9OwKaQ0w4V"
      },
      "outputs": [],
      "source": [
        "# Plot comparative analysis\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Performance metrics\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "performance_data = supervised_results[['Model'] + metrics].set_index('Model')\n",
        "performance_data.plot(kind='bar', ax=ax1)\n",
        "ax1.set_title('Performance Metrics Comparison')\n",
        "ax1.set_ylim(0.9, 1.0)\n",
        "\n",
        "# Resource usage\n",
        "resources = ['Memory Usage (MB)', 'Training Time (s)']\n",
        "resource_data = supervised_results[['Model'] + resources].set_index('Model')\n",
        "resource_data.plot(kind='bar', ax=ax2)\n",
        "ax2.set_title('Resource Usage Comparison')\n",
        "\n",
        "# CPU usage\n",
        "cpu_metrics = ['Peak CPU Usage (%)', 'Average CPU Usage (%)']\n",
        "cpu_data = supervised_results[['Model'] + cpu_metrics].set_index('Model')\n",
        "cpu_data.plot(kind='bar', ax=ax3)\n",
        "ax3.set_title('CPU Usage Comparison')\n",
        "\n",
        "# Cross-validation comparison\n",
        "cv_data = supervised_results[['Model', 'Cross Validation Mean']].set_index('Model')\n",
        "cv_data.plot(kind='bar', ax=ax4, legend=False)\n",
        "ax4.set_title('Cross-Validation Performance')\n",
        "ax4.set_ylim(0.98, 1.0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SitJnqly0w2x",
      "metadata": {
        "id": "SitJnqly0w2x"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 6452957,
          "sourceId": 10450388,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30822,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 7105.621826,
      "end_time": "2025-03-09T12:08:43.247086",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-03-09T10:10:17.62526",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}